{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "## https://colab.research.google.com/\n",
    "## https://github.com/focods/ds_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/FevetS/ds_random_forest/master/media/bobross_dt.PNG\"  style=\"width: 50%; height: 50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Random Forests?\n",
    "### 1. Easy - little or no tuning, handles missing values, no need to normalize variables\n",
    "### 2. Versatile - classification, regression, binary probability, variable selection\n",
    "### 3. Decent Accuracy - often as good as more complex and harder to use models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Random Forests?\n",
    "## Just a bunch of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone knows decision trees. They're just binary flow charts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/theoatmeal-img/comics/selfie_stick/selfie_stick.jpg\" style=\"width: 50%; height: 50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot = graphviz.Digraph(comment='Should you buy a selfie stick?')\n",
    "dot.node('A', 'given up on life?')\n",
    "dot.node('B', 'YES:\\nbuy a selfie stick')\n",
    "dot.node('J', 'NO:\\nJapanese?')\n",
    "dot.node('C', 'YES:\\nbuy a selfie stick')\n",
    "dot.node('D', \"NO:\\ndon't buy a selfie stick\")\n",
    "\n",
    "dot.edges(['AB', 'AJ', 'JC', 'JD'])\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are splits made in classification?\n",
    "**Gini Impurity**  \n",
    "The probability of an incorrect classification for a new sample given the distribution of samples in the set.\n",
    "\n",
    "\\begin{align}\\operatorname{I}_{G}(p) = \\sum_{i=1}^{J} p_i (1-p_i) \n",
    "\\end{align}\n",
    "\n",
    "where, $p_i$ is the fraction of samples with class i from J classes. \n",
    "\n",
    "* Gini has a minimum of 0 when all samples are in a single class.  \n",
    "* It approaches a maximum of 1 as samples are evenly distributed among an infinite number of classes.\n",
    "\n",
    "In a decision tree, splitting at a node is determined by the criteria that causes the greatest decrease in Gini from the set of samples before the split to the weighted average of Gini of both branches after the split.\n",
    "\n",
    "Entropy (or Information Gain) can be used instead of Gini but is more computationaly expensive and produces nearly the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quit a few parameters for tuning the tree\n",
    "1. Maximum depth\n",
    "2. Minimum number of samples to split at a node\n",
    "3. Minimum number of samples in a leaf\n",
    "4. Maximum number of features to consider at a node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic example with the iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data set and train/test split\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A complex decision tree with default settings\n",
    "def_clf = tree.DecisionTreeClassifier()\n",
    "def_clf = def_clf.fit(X_train, y_train)\n",
    "\n",
    "def_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple decision tree with max_depth=2\n",
    "simp_clf = tree.DecisionTreeClassifier(max_depth=2)\n",
    "simp_clf = simp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do the trees look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shrink svg to make them more visible\n",
    "# from IPython.display import HTML\n",
    "# style = \"<style>svg{width:20% !important;height:20% !important;}</style>\"\n",
    "# HTML(style);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default tree\n",
    "dot_data = tree.export_graphviz(def_clf, out_file=None, \n",
    "                     feature_names=iris.feature_names[:2],  \n",
    "                     class_names=iris.target_names,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simple tree\n",
    "dot_data = tree.export_graphviz(simp_clf, out_file=None, \n",
    "                     feature_names=iris.feature_names[:2],  \n",
    "                     class_names=iris.target_names,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the effect of a complex tree on prediction?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def decision_surface(ax, clf, X, y, title):\n",
    "    axx, axy = X[:, 0], X[:, 1]\n",
    "    axx_min, axx_max = axx.min() - 1, axx.max() + 1\n",
    "    axy_min, axy_max = axy.min() - 1, axy.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(axx_min, axx_max, 0.02),\n",
    "                         np.arange(axy_min, axy_max, 0.02))\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(axx, axy, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set(xlim=[xx.min(), xx.max()], ylim=[yy.min(), yy.max()],\n",
    "           xlabel='Sepal length', ylabel='Sepal width', title=title)\n",
    "    return ax\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (8,8))\n",
    "decision_surface(axs.flat[0], def_clf, X_train, y_train, title='Default - Training')\n",
    "decision_surface(axs.flat[1], def_clf, X_test, y_test, title='Default - Test')\n",
    "decision_surface(axs.flat[2], simp_clf, X_train, y_train, title='Simple - Training')\n",
    "decision_surface(axs.flat[3], simp_clf, X_test, y_test, title='Simple - Test')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complex decision tree with default settings partitions the feature space too finely and doesn't generalize well to the test data (i.e. low bias, high variance). The simplified tree generalizes better to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a confusion matrix\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def pretty_matrix(y_true, y_pred, names):\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    cm = pd.DataFrame(cm, index = names, columns= names)\n",
    "    cm.index.name = \"Observed\"\n",
    "    cm.columns.name = \"Predicted\"\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"viridis\", ax=ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of default model\n",
    "y_pred = def_clf.predict(X_test)\n",
    "fig = pretty_matrix(y_test, y_pred, iris.target_names)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "print(\"Overall Accuracy=\", metrics.accuracy_score(y_test, y_pred).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of simple model\n",
    "y_pred = simp_clf.predict(X_test)\n",
    "fig = pretty_matrix(y_test, y_pred, iris.target_names)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "print(\"Overall Accuracy=\", metrics.accuracy_score(y_test, y_pred).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this effect of overfitting when comparing the accuracy statistics as well. The simplied decision tree has higher overall accuracy and yields better precision and recall for the two classes with less support.  \n",
    "\n",
    "In practice tuning a decision tree to reduce overfitting while maintaining high accuracy can be challenging. Overfitting can also be reduced by \"pruning\" nodes that don't significantly improve the accuracy of tree after the model has already been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro: Easy to interpret, no data prep needed, use for classification or regression, etc.**  \n",
    "\n",
    "**Con: Prone to overfitting and often difficult to achieve high accuracies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hence Random Forests..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests solves the overfitting problem and improves accuracy by aggregating predictions from an ensemble of decision trees which incorporate randomness in two ways:  \n",
    "\n",
    "*The Randomness in Random Forests*\n",
    "1. Bagging - Each tree is trained with a random sample of the data that is taken with replacement (usually the size of the original dataset), which causes about 1/3 of the samples to be excluded from each tree. This bootstrapped sampling decorrelates the trees which reduces overall model variance.  \n",
    "\n",
    "\n",
    "2. Subset of features - Splitting at each node considers a random subset of all the features (usually the square root of the number of features). This also decorrelates trees by preventing dominance of the same features in every tree.\n",
    "\n",
    "*Aggregating Trees*  \n",
    "Each tree is typically grown to full depth with no pruning. Model bias is increased and variance is reduced by aggregating multiple trees.  \n",
    "* Classification - simple or weighted vote counting  \n",
    "* Regression - simple or weighted mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/06/random-forest7.png\" style=\"width: 50%; height: 50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Revisit with Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how this method performs on the Iris dataset we previously looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A complex decision tree with default settings\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100) # 100 will be the new default soon\n",
    "rf_clf = rf_clf.fit(X_train, y_train)\n",
    "\n",
    "rf_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of default random forest model\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "fig = pretty_matrix(y_test, y_pred, iris.target_names)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "print(\"Overall Accuracy=\", metrics.accuracy_score(y_test, y_pred).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already a small improvement over the previous models. We're only using two features so random selection in node splitting isn't benefitting the model as much in this example as it would for most real world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one of the trees\n",
    "dot_data = tree.export_graphviz(rf_clf.estimators_[0], out_file=None, \n",
    "                     feature_names=iris.feature_names[:2],  \n",
    "                     class_names=iris.target_names,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree becomes different from our previous decision tree after the first split. Since trees are decorrelated they will each be a little different and looking at individual trees may not help much with understanding the overall model. So we lose a lot of the interpretability of a simple decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Land cover classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris is great for simple examples, but to get a better sense of how to work with random forest lets use a slightly more complex dataset by doing land cover classification from satellite imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Landsat 8 - Operational Land Imager*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://prd-wret.s3-us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/styles/atom_page_medium/public/thumbnails/image/Landsat_8_%28LDCM%29_Satellite_over_Earth%2C%20Wiki%20Commons.jpg?itok=8broIIlp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Landsat 8 Spectral Response*\n",
    "\n",
    "<img src=\"https://github.com/FevetS/ds_random_forest/raw/master/media/l8_spectral_response.PNG\">\n",
    "\n",
    "https://landsat.usgs.gov/spectral-characteristics-viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and train test split\n",
    "data = pd.read_csv(r\"https://raw.githubusercontent.com/FevetS/ds_random_forest/master/media/MN_l8_2013med_samp_filt.csv\")\n",
    "\n",
    "X = data.drop(['landcover'], axis=1)\n",
    "y = data['landcover']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['landcover'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pairplot of original bands colored by class\n",
    "bands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "sns.pairplot(train[bands + ['landcover']], hue='landcover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the original bands are highly correlated, and they don't seperate the classes very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Spectral Indices*\n",
    "\n",
    "Normalized Difference Vegetation Index (NDVI) = (NIR - Red)/(NIR + Red)  \n",
    "\n",
    "Normalized Burn Ratio (NBR) = (NIR - SWIR2)/(NIR + SWIR2)  \n",
    "\n",
    "Normalized Difference Moisture Index (NDMI) = (NIR - SWIR1)/(NIR + SWIR1)  \n",
    "\n",
    "Brightness, Greenness, Wetness: Tasseled-Cap Indices, which are basically the first three principal components of the original band values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot of features colored by class\n",
    "specix = ['ndvi', 'nbr', 'ndmi', 'greenness', 'wetness', 'brightness']\n",
    "sns.pairplot(train[specix + ['landcover']], hue='landcover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True).fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "fig = pretty_matrix(y_test, y_pred, np.unique(y_test))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(\"Overall Accuracy=\", metrics.accuracy_score(y_test, y_pred).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Features of Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since random forest uses a bootstrapped sample for each tree, the withheld (or out-of-bag) samples can be used for internal cross-validation. Each sample is predicted using the trees which did not include it for training. Then the predicted values are compared against the true values.\n",
    "\n",
    "The out-of-bag error is usually a good unbiased estimator of accuracy and it can be used for model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OOB score to test score (score=fraction of samples correctly classified, i think)\n",
    "print(\"Out-of-bag error =\", rf.oob_score_.round(2))\n",
    "print(\"Test error = \", rf.score(X_test, y_test).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class prediction probability for each sample from the OOB prediction,\n",
    "# which enables calculation of more error stats if needed\n",
    "pd.DataFrame(rf.oob_decision_function_, index = y_train.index, columns = np.unique(y_train)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest can also be used find the relative importance of each feature. This is sometimes used for feature selection even if another model is ultimately used for prediction.  \n",
    "\n",
    "Feature importance is often defined as the mean decrease in Gini impurity for the feature across all trees and weighted by the number of the samples that reach the node where the feature is used. Reduction in out-of-bag error is another common method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances are an attribute of the model object\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but you can get the importance from each tree for plotting variability\n",
    "imps = [tree.feature_importances_ for tree in rf.estimators_]\n",
    "imps = pd.DataFrame(imps, columns=X_train.columns)\n",
    "ranked = imps.mean().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "sns.violinplot('variable', 'value', data=imps.melt(), order=ranked.index, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, many features are highly correlated..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(X_train.corr().round(2), annot=True, cmap=\"RdBu\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and yet their relative importance can sometimes be very different.  \n",
    "\n",
    "Random Forest will simply choose the best feature at a particular node which mean that even a very similar feature may only be used further down the tree where it causes less decrease in impurity. This can sometimes give the false impression that a feature is unimportant when it fact it may be effectively interchangeable with a very important feature. Or, two highly correlated features could have nearly the same importance and all subsequent feature importances get diluted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Random Forest  \n",
    "Including all the best parts of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Unaffected by multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlated predictors and check accuracy\n",
    "X_tcor = X_train.drop(['swir2', 'blue', 'green', 'brightness', 'nbr'], axis=1)\n",
    "rf_nocorr = RandomForestClassifier(n_estimators=100, oob_score=True).fit(X_tcor, y_train)\n",
    "\n",
    "X_vcor = X_test.drop(['swir2', 'blue', 'green', 'brightness', 'nbr'], axis=1)                                                             \n",
    "y_pcor = rf_nocorr.predict(X_vcor)\n",
    "fig = pretty_matrix(y_test, y_pcor, np.unique(y_test))\n",
    "print(metrics.classification_report(y_test, y_pcor))\n",
    "print(\"Overall Accuracy=\", metrics.accuracy_score(y_test, y_pcor).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall accuracy may actually drop a little because some explanatory power could be lost even though the dropped predictors were highly correlated with kept predictors. So correlated predictors don't often decrease the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ignores uninformative predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uninformative predictors simply don't get used in splitting because they don't decrease impurity, so they have no affect on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add useless features and check accuracy \n",
    "useless = pd.DataFrame(np.random.rand(len(X_train), 3), index=X_train.index)\n",
    "X_trand=  pd.concat([X_train, useless], axis=1)\n",
    "rf_rand = RandomForestClassifier(n_estimators=100, oob_score=True).fit(X_trand, y_train)\n",
    "\n",
    "useless = pd.DataFrame(np.random.rand(len(X_test), 3), index=X_test.index)\n",
    "X_vrand = pd.concat([X_test, useless], axis=1)                                                             \n",
    "y_prand = rf_rand.predict(X_vrand)\n",
    "print(\"Overall Accuracy=\", metrics.accuracy_score(y_test, y_prand).round(2))\n",
    "pd.Series(rf_rand.feature_importances_, index=X_trand.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the random predictors (0,1,2) have the lowest feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. No need to transform or scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale some predictors and check accuruacy and variable importance\n",
    "X_tscale = X_train.copy()\n",
    "X_tscale[specix] = X_tscale[specix] * 1e7\n",
    "\n",
    "rf_scale = RandomForestClassifier(n_estimators=100, oob_score=True).fit(X_tscale, y_train)\n",
    "\n",
    "X_vscale = X_test.copy()\n",
    "X_vscale[specix] = X_vscale[specix] * 1e7                                                           \n",
    "y_pscale = rf_scale.predict(X_vscale)\n",
    "print(\"Overall Accuracy=\", metrics.accuracy_score(y_test, y_pscale).round(2))\n",
    "# check variable importance\n",
    "pd.Series(rf_scale.feature_importances_, index=X_tscale.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and feature ranking is maintained after scaling some predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Works with categorical predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert one predictor to categorical and encode, then run model\n",
    "cat = pd.cut(X_train['ndvi'], bins=[-1,0,0.25, 1], labels=['l','m','h'])\n",
    "encoded = pd.get_dummies(cat, prefix='ndvi')\n",
    "X_tcat = pd.concat([X_train.drop('ndvi', axis=1), encoded], axis=1)\n",
    "\n",
    "rf_cat = RandomForestClassifier(n_estimators=100, oob_score=True).fit(X_tcat, y_train)\n",
    "\n",
    "cat = pd.cut(X_test['ndvi'], bins=[-1,0,0.25, 1], labels=['l','m','h'])\n",
    "encoded = pd.get_dummies(cat, prefix='ndvi')\n",
    "X_vcat = pd.concat([X_test.drop('ndvi', axis=1), encoded], axis=1)                                                          \n",
    "y_pcat = rf_cat.predict(X_vcat)\n",
    "\n",
    "print(\"Overall Accuracy=\", metrics.accuracy_score(y_test, y_pcat).round(2))\n",
    "pd.Series(rf_cat.feature_importances_, index=X_tcat.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't complain about the one-hot-encoded predictors, but they lose some explanatory power and a correlated predictor takes their place in the feature ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tuning often not necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest tends to work very well with default settings, but if you want to eak out another 0.5% increase in accuracy you can try tweaking:\n",
    "1. n_estimators - the number of trees\n",
    "2. max_features - the number of variables considered at each node\n",
    "3. any of the decision tree parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initiate the parameter grid \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = {\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'n_estimators': [10, 20, 50, 100, 200, 300, 500]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(),\n",
    "                           param_grid = grid, cv = 3, n_jobs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = results[(results.param_max_features=='sqrt') & (results.param_min_samples_leaf==1)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.errorbar(sub['param_n_estimators'], sub['mean_test_score'], yerr=sub['std_test_score'])\n",
    "ax.set(xlabel='n_estimators', ylabel='mean_test_score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change in accuracy is pretty minimal after 100 trees, which is a standard value.\n",
    "\n",
    "\n",
    "I think the primary benefit of tuning for Random Forest is actually for reducing processing time for large datasets. Reducing the number of trees or the complexity of the trees can save time for training and prediction. \n",
    "\n",
    "For example, training time scales linearly with the number of trees. Training takes 5 times as long for 500 trees as it does for 100 trees, but accuracy doesn't improve at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.errorbar(sub['param_n_estimators'], sub['mean_fit_time'], yerr=sub['std_fit_time'])\n",
    "ax.set(xlabel='n_estimators', ylabel='mean_fit_time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the out-of-bag errors to evaluate the model in the grid search, so the data doesn't need to be further divided.  \n",
    "But, we don't easily get the fancy results dictionary provided by GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the grid and use OOB to get the best params\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "rfg = RandomForestClassifier(oob_score=True)\n",
    "best_score=0\n",
    "for g in ParameterGrid(grid):\n",
    "    rfg.set_params(**g)\n",
    "    rfg.fit(X_train, y_train)\n",
    "    # save if best\n",
    "    if rfg.oob_score_ > best_score:\n",
    "        best_score = rfg.oob_score_\n",
    "        best_grid = g\n",
    "\n",
    "print(\"OOB accuracy=\", best_score.round(2))\n",
    "print(\"Grid:\", best_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Supposed to work with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART models can handle missing values by choosing a surrogate split, which means Random Forest should have the same ability. Unfortunately, I can't find an implementation which uses surrogate splits. But you might be able to roll your own with [rpart](https://cran.r-project.org/web/packages/rpart/rpart.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Robust to overfitting\n",
    "As evidenced by the OOB accuracy roughly matching the test set accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lost interpretability  \n",
    "Having multiple trees grown to full depth makes the model far too difficult to interpret, but we can get some understanding of the relative influence of the predictors by looking at variable importance (accounting for multicollinearity) and through partial dependence plots. \n",
    "\n",
    "Partial dependence plots show the marginal effect of a feature on the model prediction. The partial dependence function gives the average prediction for a particular value of our feature of interest by iterating over all other feature values through a  a Monte-Carlo simulation (I think). Unfortunately, the PDP will show some unrealistic values if the other features are correlated with our feature of interest, which is clearly a problem we have with this dataset.\n",
    "\n",
    "See this site for a much better explanation: https://christophm.github.io/interpretable-ml-book/pdp.html\n",
    "\n",
    "Scikit-learn's partial dependence functions can only be used with Gradient Boosting, but PDPbox works with Random Forest. Skater is another package for model interpretation. Also see iml or pdp in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdpbox\n",
    "from pdpbox import pdp, info_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid used to make the plot (monte-carlo sim?)\n",
    "pdp_ndvi = pdp.pdp_isolate(\n",
    "    model=rf, dataset=train, model_features=X_train.columns, feature='ndvi'\n",
    ")\n",
    "\n",
    "# make the PD plot\n",
    "fig, axes = pdp.pdp_plot(\n",
    "    pdp_isolate_out=pdp_ndvi, feature_name='ndvi', center=True,\n",
    "    ncols=3, plot_lines=True, plot_pts_dist=True, frac_to_plot=100\n",
    ")\n",
    "\n",
    "# set class names as x-labels\n",
    "for i, ax_dict in enumerate(axes['pdp_ax']):\n",
    "    ax = ax_dict['_count_ax']\n",
    "    ax.set(xlabel = \"ndvi (\" + rf.classes_[i] + \")\",\n",
    "           title = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDPbox's dependence plots show the average probablity of prediction of a class across the range of values for a particular feature (black/yellow line). It also shows the Individual Conditional Expectation (ICE plot), which has the marginal effect of the feature for all data points (thin blue lines). The lines below the x-axis is a rug plot of data points, so there is greater uncertainty in the prediction probability where there are fewer data points.\n",
    "\n",
    "Remember that NDVI is an indicator of vegetation quantity and health, thus as NDVI increases the probability of predicting a vegetation class (forest, herbaceous, or shrubs) tends to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pdp for interaction of two features\n",
    "pdp_ndvi_ndmi = pdp.pdp_interact(\n",
    "    model=rf, dataset=train, model_features=X_train.columns, features=['ndvi', 'ndmi'], \n",
    "    num_grid_points=[10, 10],  percentile_ranges=[None, None], n_jobs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the pdp interact plots\n",
    "fig, axes = pdp.pdp_interact_plot(\n",
    "    pdp_ndvi_ndmi, ['ndvi', 'ndmi'], plot_type='grid', ncols=2, plot_pdp=True, which_classes=[6, 7]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interact plot shows a heatmap of the prediction probability across the distribuation of two marginal features (ndvi and ndmi). We can see that class 6 (water) has a high prediction probability when ndmi is very high and ndvi is very low. Class 7 (wetlands) tend to be predicted when ndmi and ndvi are both moderate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Susceptible to imbalanced data  \n",
    "Like many other models, Random Forest will tend to favor an overly dominant class. You can counteract this by either setting **class weight** or through **sampling**. The dominant class can be *undersampled* or the rare classes can be *oversampled* to improve class balance. There are a variety of sampling methods for handling imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Usually not the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is great for its versatility, ease of use, and reasonable performance, which is why many people reach for it first.  \n",
    "However, if you're willing to put the time and effort into building a more complex model then you'll likely get better accuracy.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forge3",
   "language": "python",
   "name": "forge3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
